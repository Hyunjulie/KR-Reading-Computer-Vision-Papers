{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1초에 100프레임으로 타겟을 추적할 수 있는 GoTurn\n",
    "### Learning to Track at 100 FPS with Deep Regression Networks \n",
    "\n",
    "이번에 리뷰할 논문은 2016년(ECCV)에 나온 논문으로, 코드가 C++ 로 공개되어 있습니다. \n",
    "\n",
    "프로젝트 페이지는: http://davheld.github.io/GOTURN/GOTURN.html \n",
    "\n",
    "논문 링크는: http://davheld.github.io/GOTURN/GOTURN.pdf 입니다.\n",
    "\n",
    "모델의 구조 자체가 굉장히 간단해서 Pytorch 로도 쉽게 구현 할 수 있습니다.\n",
    "\n",
    "<img src=\\\"images/model.png\\\" width=\\\"800\\\" height=\\\"500\\\" />\\n\n",
    "\n",
    "** - 제시하는 문제? ** \n",
    "\n",
    "\n",
    "머신러닝을 사용하는 Object Tracking 모델들은 주로 Online 에서 트레이닝 하는 방법을 사용해서 방대한 데이터를 사용할 수 없습니다. \n",
    "이 논문에서는 오프라인에서 뉴럴네트워크를 학습시켜서 1초에 100프레임 (100FPS)으로 작동할 수 있는 모델을 제시했습니다.\n",
    "\n",
    "** - 어떻게? **\n",
    "\n",
    "간단한 Feed-Forward Network를 이용한 모델입니다.물체의 움직임과 appearance 사이의 관계를 학습해서 training set에 나타나지 않은 새로운 물체를 추적할 수 있게 했습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Intro\n",
    "\n",
    "** - Single Target Tracking 이란? ** \n",
    "\n",
    "\n",
    "비디오의 한 프레임이 제시되었을 때, 관심있는 Target 에 대해서 후속 비디오 프레임에서 이 target을 계속 찾는 것 입니다. \n",
    "\n",
    "single tracking이 기반이 되어서 자율주행의 obstacle tracking 등 다양한 시스템을 구성할 수 있습니다. \n",
    "\n",
    "** - GOTURN? ** \n",
    "\n",
    "Generic Object Tracking Using Regression Networks 의 약자로, 오프라인에서 모델을 비디오로 학습시켜서 generic 한 물체를 트래킹 할 수 있도록 만들어졌습니다. \n",
    "\n",
    "위 모델의 가장 강점은 offline에서 training 시킬 수 있어서 100PFS로 빠르게 사용할 수 있다는 것입니다. \n",
    "\n",
    "** - 그렇다면, 왜 이전 모델들은 느렸는가? ** \n",
    "\n",
    "\n",
    "  - Online 에서 트레이닝을 시켰다\n",
    "  - 주로 Classification-based 접근을 사용: 많은 image patch 를 만들어서 target을 찾는 방법으로 접근 \n",
    "    --> 여기서는 Regression-based 접근 사용: single feed-forward pass 를 이용해서 타겟의 위치로 바로 찾을 수 있도록 한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Method\n",
    "\n",
    "#### Input & Output\n",
    "- Target 물체가 무엇인지 이미지 제시: 타겟이 이미지의 중심에 오도록 전 프레임에 있는 타겟 이미지를 crop 한다 \n",
    "    - 타겟에 대한 정보를 전 프레임에서 가져오기 때문에 처음 본 타겟도 잘 찾을 수 있다 \n",
    "    - Crop 된 이미지 안에 있는 타겟을 찾도록 네트워크가 설정된다 \n",
    "    - 타겟 주변의 contextual information도 제공하기 위해서 어느정도 패드를 넣어준다 \n",
    "\n",
    "\n",
    "프레임 (t - 1) 일 때 네트워크가 예상한 bounding box의 위치를 c = (cx, cy) 라고 하자. \n",
    "프레임 t가 됐을 때, 전 프레임에서 예상한 위치 c 를 중심으로 crop 을 또 하면서 다음 프레임에게 타겟이 무엇인지 알려준다. \n",
    "\n",
    "** - 타겟의 위치를 어떻게 예상하는가? **\n",
    "\n",
    "전 프레임에서 타겟이 어디에 있었는지 본다. \n",
    "보통 물체들이 Smooth 하게 움직이기 때문에 다음 프레임에서 물체가 어디쯤에 있을지 좋은 힌트가 된다. \n",
    "\n",
    "전 프레임에서 예측한 그 위치에 전보다 k배 큰 박스를 만들어서 Crop 한다 (k = 2를 사용함. 즉, 2배 큰 박스를 만든다). \n",
    "\n",
    "타겟이 너무 빨리 움직이거나 다른 물체에 의해서 감춰지지 않는 이상 crop  된 박스 안에 물체가 있으 것이다. 빨리 움직이는 물체라면, k 를 증가시킨다. \n",
    "\n",
    "** - Output: 현재 프레임에서 물체가 있을 것이라고 예상되는 곳의 coordinates ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture \n",
    "\n",
    "인풋: 타겟, Search region \n",
    "\n",
    "Convolutional Layer --> Fully Connected Layers \n",
    "\n",
    "\n",
    "- Convolutional Layer 의 역할: 이미지의 high-level representation 뽑아내기 \n",
    "- Fully Connected Layer 의 역할: 타겟의 Feature와 현재 프레임의 feature 를 비교해서 어디있는지 예측하는 것 \n",
    "\n",
    "\n",
    "1.  Convolutional Layer:  CaffeNet의 첫 5개 Layer \n",
    "\n",
    "2.  여기서 나온 결과를 하나의 벡터로 연결한다 \n",
    "\n",
    "3.  이 벡터를 3개의 Fully Connected Layer에 넣는다 (4096nodes) \n",
    "\n",
    "4.  Output Layer: 4 nodes 인 Fully Connected Layer \n",
    "\n",
    "\n",
    "CaffeNet의 parameter를 사용하고, fully connected layer 사이에 dropout, ReLU 사용. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 정리하면, \n",
    "- 처음 인풋으로 어떤걸 타겟으로 할 것인지 직접 타겟 주변에 bounding box를 만들어서 입력해야 한다 \n",
    "\n",
    "- 내가 입력한 bounding box만 crop 해서 #1 convolution -> 그 타겟의 feature 를 뽑아내기 \n",
    " \n",
    " \n",
    "- 그 bounding box 보다 k 배 큰 박스를 다음 프레임에 같은 위치에서 crop! (보통 smooth하게 움직여서 주변에 있을 확률이 높음) 빨리 움직이는 물체가 타겟이라면 k를 크게 하면 됨 --> 이 crop 된 이미지도 #2 convolution -> 그 이미지의 feature 뽑아내기\n",
    "\n",
    "- 두 Feature map을 비교해서 타겟의 feature 와 같은 곳에 새로운 bounding box 를 만든다 \n",
    "\n",
    "- 무한반복 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Motion Smoothness \n",
    "\n",
    "보통 물체는 smooth 하게 움직이기 때문에 \n",
    "\n",
    "큰 움직임 보다는 작은 움직임을 prefer하도록 모델을 가르쳐야 한다. \n",
    "\n",
    "HOW? \n",
    "- Laplace distribution으 사용해서 random crop을 한 training set를 augment 시킨다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그 외에 ... \n",
    "- convolution layer: pretrained on ImageNet \n",
    "- No fine-tuning - 트레이닝 사이즈가 작기때문에 못함 \n",
    "- Nvidia GeForce GTX Titan X GPU (cuDNN acceleration): 165fps \n",
    "- GTX 680 GPU: 100fps\n",
    "- CPU: 2.7 fps\n",
    "\n",
    "결과: [사진 넣기]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models \n",
    "import torch.nn as nn\n",
    "\n",
    "# Most of the source code: https://github.com/amoudgl/pygoturn/blob/master/src/model.py \n",
    "\n",
    "class GoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        #nn.Module 에서 상속받도록 \n",
    "        super(GoNet, self).__init__()\n",
    "        \n",
    "        #Convolution 으로 학습된 caffenet을 그대로 썼으니까 가져오기 \n",
    "        caffenet = models.alexnet(pretrained=True)\n",
    "        for param in caffenet.parameters():\n",
    "            param.requires_grad = False  #Fine tuning 이 안되도록 모든 layer freeze 시키기\n",
    "        self.features = caffenet.features\n",
    "        \n",
    "        #Caffenet과 같은 구조로 만들기 \n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(256*6*6*2, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096,4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(4096, 4),\n",
    "                )\n",
    "        \n",
    "        self.weight_init()\n",
    "    \n",
    "    def weight_init(self):\n",
    "        for m in self.classifier.modules():\n",
    "            # Fully Connected Layer의 parameter 들은 mean=0, std=0.005, bias=1로 initialize 됨\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.bias.data.fill_(1)\n",
    "                m.weight.data.normal_(0, 0.005)\n",
    "\n",
    "    #실제로 Model 만들기!\n",
    "    def forward(self, x, y):\n",
    "        #각기 다른 2개의 stream 을 만들고\n",
    "        #합쳐(concat해)서 fully connected 에 넣기\n",
    "        x1 = self.features(x)\n",
    "        x1 = x1.view(x.size(0), 256*6*6)\n",
    "        \n",
    "        x2 = self.features(y)\n",
    "        x2 = x2.view(x.size(0), 256*6*6)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 출처: https://github.com/amoudgl/pygoturn/blob/master\n",
    "\n",
    "사실 이 GoTurn 모델 자체는 굉장히 간단하지만, 데이터 전처리 작업이 훨씬 오래걸리는 모델입니다. \n",
    "\n",
    "Convolution 에 데이터를 넣기 전에 initial Bounding Box 를 찾는 것, \n",
    "\n",
    "다음 프레임에 사용될 predicted bounding box 를 예측하는 것 등 convolution에 들어가기 전까지가 더 머리아픈 것 같습니다 ㅠㅡㅜ \n",
    "\n",
    "코드의 출처를 보시면 구현을 해놓았으니, 관심있으면 확인하세요 :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
